<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning | Ge[o]rges Dib</title>
    <link>https://dibgerge.github.io/tags/deep-learning/</link>
      <atom:link href="https://dibgerge.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>deep learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 11 Mar 2019 00:00:00 -0700</lastBuildDate>
    <image>
      <url>https://dibgerge.github.io/images/icon_hufb0bba0b24038dd6774d907c032416d0_44829_512x512_fill_lanczos_center_2.png</url>
      <title>deep learning</title>
      <link>https://dibgerge.github.io/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Tensorflow, PyTorch, and MxNet</title>
      <link>https://dibgerge.github.io/post/tf-mxnet-pytorch/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 -0700</pubDate>
      <guid>https://dibgerge.github.io/post/tf-mxnet-pytorch/</guid>
      <description>&lt;p&gt;In the past 2 years, I have been exclusively using Tensorflow with Keras for training and testing deep neural networks. I never really thought twice about it, because this is what everyone else seems to be using. I mean, just looking at the popularity of those frameworks in github compared with their closest competitors was enough to make the decision for me. Here are the approximate statistics as of March 2019 for the four most popular frameworks.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;Tensorflow&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/keras-team/keras&#34;&gt;Keras&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;Pytorch&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/apache/incubator-mxnet&#34;&gt;Mxnet&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Github &lt;i class=&#34;fas fa-star&#34;&gt;&lt;/i&gt;&lt;/td&gt;
&lt;td&gt;122,500&lt;/td&gt;
&lt;td&gt;39,000&lt;/td&gt;
&lt;td&gt;25,500&lt;/td&gt;
&lt;td&gt;16,500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Recently, I have been going through the lectures of &lt;a href=&#34;fast.ai&#34;&gt;Jeremy Howard&#39;s Fast.ai&lt;/a&gt; excellent class, where they use Pytorch as the backend of the high-level library they built. This was one of the reasons I started questioning what was I missing by limiting myself to Tensorflow. Also, I have seen the extensive vision library of &lt;a href=&#34;https://gluon-cv.mxnet.io/index.html&#34;&gt;MxNet&#39;s GluonCV&lt;/a&gt;, and this really made me feel that I might be missing out.&lt;/p&gt;
&lt;p&gt;In addition to all the fear of missing out stuff, I was coding my own implementation of &lt;a href=&#34;https://gluon-cv.mxnet.io/index.html&#34;&gt;ResNet&lt;/a&gt;, and I scoured online the plethora of different implementations of ResNet, some with lots of &lt;i class=&#34;fas fa-star&#34;&gt;s&lt;/i&gt;. I realized that many had implementation details which differed from the original paper, and it was not clear if it is intentional or not. Thus I really though that a test in reproducibility is due and so I did.&lt;/p&gt;
&lt;p&gt;I set out to compare the three frameworks: tensorflow, pytorch, and mxnet. I wanted to use some of the built-in convolutional neural networks with built-in pre-trained weights to directly classify Imagenet&#39;s validation dataset images. Throughout this comparison, I was looking for three factors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ease of loading the dataset and reproducing the pre-processing steps required before feeding images to the neural network for classification.&lt;/li&gt;
&lt;li&gt;Computed error rate vs framework&#39;s reported error. It would be also nice to compare the results with the original publication, but most report validation error using 10-crops, and here we use only single crop.&lt;/li&gt;
&lt;li&gt;Inference speed.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;dataset-and-convolutional-neural-network-architectures&#34;&gt;Dataset and Convolutional Neural Network Architectures&lt;/h2&gt;
&lt;p&gt;The datatset I used was Imagenet&#39;s validation dataset which constitutes of 50,000 images with different sizes. The easiest way to get your hands on the data is to download it from Kaggle &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the frameworks, I used the following versions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tensorflow 1.13&lt;/strong&gt; with Keras as the front-end interface (Note this is different from using Keras with Tensorflow backend since now Tensorflow comes packed with its own Keras version, which is not compatible with other frameworks).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.0.1&lt;/strong&gt; with PyTorch vision for convolutional neural networks implementations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MxNet 1.4&lt;/strong&gt; with GluonCV for convolutional neural networks implementaitons&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the convolutional neural network, I considered four different architectures: &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;ResNet50&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;Resnet101&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;Mobilenet&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/1608.06993&#34;&gt;Densenet121&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The top 1 error published on each frameworks&amp;rsquo; website, based on a single center crop evaluation is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/keras-team/keras-applications&#34;&gt;Tensorflow (Keras)&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34;&gt;Pytorch&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://gluon-cv.mxnet.io/model_zoo/classification.html&#34;&gt;Mxnet (GluonCV)&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet50&lt;/td&gt;
&lt;td&gt;25.10&lt;/td&gt;
&lt;td&gt;23.85&lt;/td&gt;
&lt;td&gt;22.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101&lt;/td&gt;
&lt;td&gt;23.60&lt;/td&gt;
&lt;td&gt;22.63&lt;/td&gt;
&lt;td&gt;21.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mobilenet&lt;/td&gt;
&lt;td&gt;29.60&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;26.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Densenet121&lt;/td&gt;
&lt;td&gt;25.0&lt;/td&gt;
&lt;td&gt;25.35&lt;/td&gt;
&lt;td&gt;25.03&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We note here that the error for Resnet101 is reported in Keras Applications, but Resnet101 is not packaged with Tensorflow 1.13, which is used in this experiment, and thus we won&#39;t have any results for it. Also, Pytorch does not come packaged with an implementation of Mobilenet.&lt;/p&gt;
&lt;p&gt;Also, it is interesting to see that especially for Resnet implementations, it is obvious that the accuracy of Mxnet &amp;gt; PyTorch &amp;gt; Tensorflow.&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;All the code used here can be found on &lt;a href=&#34;https://github.com/dibgerge/blog-tf-mxnet-pytorch&#34;&gt;github&lt;/a&gt;. The biggest challenge here really is loading and pre-processing the images the right way. The following briefly describes how I did it for each of the three frameworks.&lt;/p&gt;
&lt;h3 id=&#34;tensorflow-pre-processing&#34;&gt;Tensorflow pre-processing&lt;/h3&gt;
&lt;p&gt;Uses the &lt;code&gt;Sequence&lt;/code&gt; abstract class for loading the data using a a generator. The pre-processing pipeline is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resized the shorter side of the image to 256 pixels (implemented manually).&lt;/li&gt;
&lt;li&gt;Center cropped the image to 224 x 224 pixels (implemented manually).&lt;/li&gt;
&lt;li&gt;Apply the function &lt;code&gt;preprocess_input&lt;/code&gt; which applies the appropriate normalization based on how the network is trained. This function is provided with each different architecture built-in Tensorflow&#39;s Keras frontend.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pytorch-pre-processing&#34;&gt;Pytorch pre-processing&lt;/h3&gt;
&lt;p&gt;Used the &lt;code&gt;Dataset&lt;/code&gt; and &lt;code&gt;Dataloader&lt;/code&gt; interfaces to feed the data to the neural network. The standard pipeline for pre-processing images for all architectures is documented and is a composition of four transforms using Pytorch&#39;s &lt;code&gt;transforms&lt;/code&gt; API:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Resize image to 256 pixels (using &lt;code&gt;transforms.Resize&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Center image to 224 pixels (using &lt;code&gt;transforms.CenterCrop&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Convert images to pytorch tensor (using &lt;code&gt;transforms.ToTensor&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Normalize image by mean subtraction and standard deviation scaling (using &lt;code&gt;transforms.Normalize&lt;/code&gt;). The normalization values are given in the Pytorch&#39;s documentation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;mxnet-pre-processing&#34;&gt;Mxnet pre-processing&lt;/h3&gt;
&lt;p&gt;Used the &lt;code&gt;Dataset&lt;/code&gt; and &lt;code&gt;Dataloader&lt;/code&gt; interfaces to feed the data to the neural network. Mxnet comes packaged with a preset function which applies the required pipeline for models pre-trained on imagenet, using the &lt;code&gt;gluoncv.data.transforms.presets.imagenet.transform_eval&lt;/code&gt; function.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The following table summarizes the achieved error percentages for each of the four architectures using the three different frameworks.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;(% error)&lt;/th&gt;
&lt;th&gt;Tensorflow&lt;/th&gt;
&lt;th&gt;Pytorch&lt;/th&gt;
&lt;th&gt;Mxnet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet50&lt;/td&gt;
&lt;td&gt;26.92&lt;/td&gt;
&lt;td&gt;23.87&lt;/td&gt;
&lt;td&gt;22.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;22.63&lt;/td&gt;
&lt;td&gt;21.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mobilenet&lt;/td&gt;
&lt;td&gt;29.91&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;26.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Densenet121&lt;/td&gt;
&lt;td&gt;26.69&lt;/td&gt;
&lt;td&gt;25.57&lt;/td&gt;
&lt;td&gt;25.10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interestingly, although I had the most experience with Keras, I could not reproduce the published results for Resnet and Densenet. This is probably because I had to manually implement the image resizing and cropping, which might be different one used to generate the published results. On the other hand, Pytorch provides the required documentation to be able to reproduce the results very closely. MxNet takes it even a step further by providing a single function which applies all required pre-processing, and the results match very well with published ones.&lt;/p&gt;
&lt;p&gt;As for running times, I measured the total running time for classifying 50,000 images.  All predictions were made on GPU (Nvidia GTX 1080 Ti), while the image pre-processing was made on CPU. The following tables summarizes the running time results in seconds. &lt;strong&gt;Please take those results with a huge grain of salt, since times vary a lot when loading and reloading large data from disk&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;(seconds)&lt;/th&gt;
&lt;th&gt;Tensorflow&lt;/th&gt;
&lt;th&gt;Pytorch&lt;/th&gt;
&lt;th&gt;Mxnet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet50&lt;/td&gt;
&lt;td&gt;137&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;159&lt;/td&gt;
&lt;td&gt;214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mobilenet&lt;/td&gt;
&lt;td&gt;82&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;118&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Densenet121&lt;/td&gt;
&lt;td&gt;173&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;195&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First, those results are somehow unfair to tensorflow. This is because I used threading with 4 workers for pre-processing images in Tensorflow, while 4 workers with multiprocessing was used for MxNet and Pytorch. But, this is rather Tensorflow/keras fault due to a &lt;a href=&#34;https://github.com/keras-team/keras/issues/10855&#34;&gt;bug&lt;/a&gt; not allowing me to use multiprocessing. Also, for Mxnet and Pytorch I measured the average time it took to classify 10 images (the batch size used). I did not do this in Tensorflow, because there is no easy way to measure per-batch times.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;(milliseconds)&lt;/th&gt;
&lt;th&gt;Tensorflow*&lt;/th&gt;
&lt;th&gt;Pytorch&lt;/th&gt;
&lt;th&gt;Mxnet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet50&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;6.7 $\pm$ 4.3&lt;/td&gt;
&lt;td&gt;12.1 $\pm$ 1.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;11.8 $\pm$ 1.2&lt;/td&gt;
&lt;td&gt;21.0 $\pm$ 3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mobilenet&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;6.7 $\pm$ 1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Densenet121&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;15.6 $\pm$ 1.7&lt;/td&gt;
&lt;td&gt;26.7 $\pm$ 3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most of the time here is being spent loading the data from disk, and the GPU is not fully utilized. For example, based on average inference of 10 images in Resnet50, it takes about 33.5 seconds to classify all 50,000 images using PyTorch and about 60.5 seconds using Mxnet. We can reduce total inference by more than half if the GPU is kept busy.
Thus, the importance of handling data appropriately if speed matters.&lt;/p&gt;
&lt;h2 id=&#34;who-is-the-winner&#34;&gt;Who is the winner?&lt;/h2&gt;
&lt;p&gt;I think this experiment allowed me to assess three things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Availability of pre-trained networks&lt;/strong&gt;: trophy goes to &lt;em&gt;Mxnet&lt;/em&gt;. The extensive vision library of architectures/preset image processing is very impressive. Jury still out on other domains (RL, NLP). Tensorflow is a second, and Pytorch did not have much architecture packaged with it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: Again, trophy to &lt;em&gt;Mxnet&lt;/em&gt;, with PyTorch a close second. I could not reproduce Tensorflow&#39;s results, and it is not clear how to do it using the documentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data preparation&lt;/strong&gt;: a tie with &lt;em&gt;Mxnet and Pytorch&lt;/em&gt;. Both frameworks use very similar &lt;code&gt;Dataset&lt;/code&gt; and &lt;code&gt;Dataloader&lt;/code&gt; interfaces. I was able to get going using this interface very fast. Keras also has a very similar interface using &lt;code&gt;Sequence&lt;/code&gt; class, but the absence of something similar to &lt;code&gt;Dataloader&lt;/code&gt; results in having to return batches rather than simply one image at a time. Also Tensorflow has a &lt;code&gt;Dataset&lt;/code&gt; interface which I used before but somehow made me feel I am programming in C all over again.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;: &lt;em&gt;Pytorch&lt;/em&gt;. It is very obvious that Pytorch won the speed race all over the board. Also, although there is no information for only inference time in Tensorflow, it looks like Tensorflow also has the edge on Mxnet.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After this quick experiment, I am really excited about working with Mxnet and see how it goes. It looks like a great choice for working with computer vision, even though those experiments show it is the slowest. I am excited to see if it is really worth ditching Tensorflow for it, as I test it for different deep learning applications.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;fab fa-github-square&#34; href=&#34;https://github.com/dibgerge/blog-tf-mxnet-pytorch&#34;&gt;Get the code!&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Interactive Machine Learning</title>
      <link>https://dibgerge.github.io/project/niml/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 -0700</pubDate>
      <guid>https://dibgerge.github.io/project/niml/</guid>
      <description>&lt;p&gt;Machine intelligence advances have elevated the potential of computer-based video analytics but machines alone still fall short of human visual recognition capabilities. Humans, however, cannot address the high volumes of data in need of analysis. Despite exponential growth in machine analytics, opportunities for improvement remain (reducing training sets, improving cross-platform compatibility, adapting to changing environments, etc.). This project aims at achieving higher performance thresholds by integrating the strengths of human and machine reasoning.&lt;/p&gt;
&lt;p&gt;We are currently building brain-computer interfaces and machine learning approaches to quantify neurological data reflecting fast visual recognition collected with an electroencephalogram (EEG) headset worn by a human participant viewing video data. The quality of psychological engagement are being investigated and the quality of EEG signatures are being evaluated based on reproducibility and accuracy.&lt;/p&gt;
&lt;p&gt;The NIML approach is envisaged to significantly enhance data processing with application to HIL processes that could benefit big data problems such as data labeling for medicine, social media, among others.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
